---
title: "How do emojis work?"
date: "2023-02-30"
description: "How does your computer display emojis ðŸ¤£? How does it display anything?? ðŸ’©"
tags:
  - "Computer Science"
slug: how-do-emojis-work
draft: true
banner: "src/assets/how-do-emojis-work/banner.jpg"
---

import { ByteGroup, BinaryNotepad } from "./components/binary"
import { Quote } from "ðŸ§±/article/Quote"
import T from "ðŸ§±/T.astro"
import WideMedia from "ðŸ§±/article/WideMedia.astro"
import Youtube from "ðŸ§±/article/Youtube.astro"
import Caption from "ðŸ§±/Caption.astro"


Have you ever run into this situation? You have a friend who has a different brand of phone than you and they send you a message like:

{<Quote>
Bro I totally flunked that exam man ðŸ˜­ðŸ˜­{'\u25A1'}{'\u25A1'}ðŸ˜­ 
</Quote>}

Then you go:

{<Quote>
Wait we had an exam?? Also what's that between the crying faces?
</Quote>}

And then they send you a screenshot of what the emoji looks like and proceed to berate you for owning whatever brand of phone you have (though let's be honest, it's probably a Samsung or generic Android).

You look online and find out that a whole bunch of new emojis have just been released! But you can't send them on your phone and you can't view them either. How come your friend can use new emojis but you can't? Also what _are_ those squares, and will there be a make up exam?

## Once upon a binary ðŸ¤”

Even if you're not a big tech person, chances are you've heard that computers work with just 1s and 0s, called <T>binary</T>. No spaces, nothing in between just one gigantic long strip of numbers we refer to as <T>bits</T>.

<ByteGroup text="Hi!" />

Of course, they're not really numbers, they're low and high electrical charges. But for our purposes, there's no difference between the two. These numbers eventually come to represent anything from the text on your screen to games and movies and pretty much anything you can imagine.  If you're like me, that might not make a ton of sense if it's the first time you're hearing.

{<Quote author="Me, but probably also you.">
Now hold on, only 1s and 0s? I'm reading this blog post on a machine that only understands 1s and 0s? How could that possibly be?
</Quote>}

One question you might have at this point is "Why do computers use binary, base 2, instead of base 10 like we humans do". And to answer that, let me flip the question around for you. Why do <T>we</T> use base 10 and not base 2? Maybe you thought that throughout the years we've done research to come up with the most efficient counting method and stuck with it. But unfortunately the reality is that we simply have 10 fingers and it's easy to count on them, that's mostly all there is to it. There are other civilizations like the Sumerians in ancient Mesapotamia who used base 60 instead of base 10 (you can thank them for your 60 second minutes and 60 minute hours). But they weren't any more enlightened than everyone else, they just counted different parts of their fingers.

A similar story goes for computers, although admittedly there's been a little bit more thought put into the process. What we describe as 1s and 0s are technically just high and low electrical voltages. And while base 2 isn't the _only_ way, but it does seem to be the most _reliable_ way of modeling that. Which is something we've come to expect from all computers. So base 2 it is.

{<WideMedia>
  <Youtube id="fXwSFhUVFmE" />
</WideMedia>}

{<Caption>A better explanation for why binary works well with computers.</Caption>}

## How computers read numbers ðŸ”¢

<WideMedia>
  <BinaryNotepad value="example" client:visible />
</WideMedia>
